full_accessed_data <- full_accessed_data[order(-full_accessed_data[,2]),]
new_merge <- merge(police_access, police_report, by = "report_id", all.y = TRUE)
#new_merge$created.y <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$created.y)
new_merge$created.y <- as.POSIXct(new_merge$created.y)
#new_merge$accessed <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$accessed)
new_merge$accessed <- as.POSIXct(new_merge$accessed)
new_merge$force <- gsub(" \\[UK\\]", "", new_merge$force)
for (i in 1:nrow(new_merge)) {
if (new_merge[i,16] == 1) {
new_merge[i,25] <- 'metropolitan hate crime'
} else if (new_merge[i,16] == 2) {
new_merge[i,25] <- 'sussex hate crime'
}
}
View(x)
View(y)
View(y_1)
View(x_1)
View(to.date.data)
police_rep_online <- getURL('http://dev.myevidence.org/utilities/police_data_access.php', userpwd = 'todd:Ainem2eigh')
write(police_rep_online, file = 'online_database.csv')
police_report <- read.csv('online_database.csv')
police_access_online <- getURL('http://dev.myevidence.org/utilities/police_access_access.php', userpwd = 'todd:Ainem2eigh')
write(police_access_online, file = 'online_accesslog.csv')
police_access <- read.csv('online_accesslog.csv')
## Clean police access
police_access <- police_access[!grepl("gt.witnessconfident@gmail.com", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[!grepl("geoff.tibbs@witnessconfident.org", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[!grepl("selfevident@justevidence.org", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[grepl("^used_.*", police_access$password, ignore.case = TRUE),]
police_access <- subset(police_access, !duplicated(police_access[,2]))
x <- merge(police_access, police_report, by = "report_id", all.y = TRUE)
x$force <- gsub(" \\[UK\\]", "", x$force)
for (i in 1:nrow(x)) {
if (x[i,16] == 1) {
x[i,25] <- 'metropolitan hate crime'
} else if (x[i,16] == 2) {
x[i,25] <- 'sussex hate crime'
}
}
x_1 <- count(x, "force")
colnames(x_1)[2] <- "Number Reported"
y <- merge(police_access, police_report, by = 'report_id', all.x = TRUE)
y$force <- gsub(" \\[UK\\]", "", y$force)
y$where_you_live[is.na(y$where_you_live)] <- 0
for (i in 1:nrow(y)) {
if (y[i,16] == 1) {
y[i,25] <- 'metropolitan hate crime'
} else if (y[i,16] == 2) {
y[i,25] <- 'sussex hate crime'
}
}
y_1 <- count(y, "force")
colnames(y_1)[2] <- "Number Accessed"
access_data <- merge(x_1, y_1, by = "force", all.x = TRUE)
access_data[is.na(access_data)] <- 0
percent1 <- round((access_data$`Number Accessed`/access_data$`Number Reported`) * 100)
access_data$`Percent Accessed`  <- paste(percent1, '%', sep = "")
access_data <- access_data[order(-access_data[,3]),]
access_data <- access_data[!grepl('^$', access_data$force),]
x$created.y <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", x$created.y)
for (i in 1:nrow(x)) {
if (x[i, 10] >= new_quarter) {
x[i,10] <- "Quarter to Date"
} else if (old_quarter <= x[i, 10] & x[i,10] < new_quarter) {
x[i,10] <- "Past Quarter"
} else {
x[i,10] <- "All-Time"
}
}
present_quarter <- x[grep("Quarter to Date", x$created.y),]
report1 <- count(present_quarter, "force")
colnames(report1)[2] <- "Reports Quarter to Date"
present_access <- present_quarter[!is.na(present_quarter$accessed),]
access1 <- count(present_access, "force")
colnames(access1)[2] <- "Accessed Quarter to Date"
to.date.data <- merge(report1, access1, by = "force", all.x = TRUE)
to.date.data[is.na(to.date.data)] <- 0
percent2 <- round((to.date.data$`Accessed Quarter to Date`/to.date.data$`Reports Quarter to Date`) * 100)
to.date.data$`Percent Accessed`  <- paste(percent2, '%', sep = "")
to.date.data <- to.date.data[order(-to.date.data[,3]),]
to.date.data <- to.date.data[!grepl('^$', to.date.data$force),]
new.data <- merge(access_data, to.date.data, by = 'force', all = TRUE)
past_quarter <- x[grep("Past Quarter", x$created.y),]
report2 <- count(past_quarter, "force")
colnames(report2)[2] <- "Reports Past Quarter"
past_access <- past_quarter[!is.na(past_quarter$accessed),]
access2 <- count(past_access, "force")
colnames(access2)[2] <- "Accessed Past Quarter"
past.data <- merge(report2, access2, by = "force", all.x = TRUE)
past.data[is.na(past.data)] <- 0
percent3 <- round((past.data$`Accessed Past Quarter`/past.data$`Reports Past Quarter`) * 100)
past.data$`Percent Accessed`  <- paste(percent3, '%', sep = "")
past.data <- past.data[order(-past.data[,3]),]
past.data <- past.data[!grepl('^$', past.data$force),]
full_accessed_data <- merge(new.data, past.data, by = 'force', all = TRUE)
full_accessed_data$`Number Not Accessed` <- full_accessed_data$`Number Reported` - full_accessed_data$`Number Accessed`
percent4 <- round((full_accessed_data$`Number Not Accessed` / full_accessed_data$`Number Reported`) * 100)
full_accessed_data$`Percent Not Accessed`  <- paste(percent4, '%', sep = "")
new_merge <- merge(police_access, police_report, by = "report_id", all.y = TRUE)
#new_merge$created.y <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$created.y)
new_merge$created.y <- as.POSIXct(new_merge$created.y)
#new_merge$accessed <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$accessed)
new_merge$accessed <- as.POSIXct(new_merge$accessed)
new_merge$force <- gsub(" \\[UK\\]", "", new_merge$force)
for (i in 1:nrow(new_merge)) {
if (new_merge[i,16] == 1) {
new_merge[i,25] <- 'metropolitan hate crime'
} else if (new_merge[i,16] == 2) {
new_merge[i,25] <- 'sussex hate crime'
}
}
new_merge$`access time` <- new_merge$accessed-new_merge$created.y
time.avg <- aggregate(`access time`~ force, data = new_merge, FUN = function(x) median = median(x))
time.avg$`access time` <- round(time.avg$`access time`)
colnames(time.avg)[2] <- "Time til Accessed (Minutes)"
full_accessed_data <- merge(full_accessed_data, time.avg, by = "force", all = TRUE)
full_accessed_data <- full_accessed_data[order(-full_accessed_data[,2]),]
colnames(full_access
ed_data)[c(4,7,10)] <- c("Percent Accessed", "Percent Accessed Quarter to Date", "Percent Accessed Past Quarter")
View(time.avg)
View(new_merge)
colnames(full_accessed_data)
police_rep_online <- getURL('http://dev.myevidence.org/utilities/police_data_access.php', userpwd = 'todd:Ainem2eigh')
write(police_rep_online, file = 'online_database.csv')
police_report <- read.csv('online_database.csv')
police_access_online <- getURL('http://dev.myevidence.org/utilities/police_access_access.php', userpwd = 'todd:Ainem2eigh')
write(police_access_online, file = 'online_accesslog.csv')
police_access <- read.csv('online_accesslog.csv')
## Clean police access
police_access <- police_access[!grepl("gt.witnessconfident@gmail.com", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[!grepl("geoff.tibbs@witnessconfident.org", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[!grepl("selfevident@justevidence.org", police_access$email_used, ignore.case = TRUE),]
police_access <- police_access[grepl("^used_.*", police_access$password, ignore.case = TRUE),]
police_access <- subset(police_access, !duplicated(police_access[,2]))
x <- merge(police_access, police_report, by = "report_id", all.y = TRUE)
x$force <- gsub(" \\[UK\\]", "", x$force)
for (i in 1:nrow(x)) {
if (x[i,16] == 1) {
x[i,25] <- 'metropolitan hate crime'
} else if (x[i,16] == 2) {
x[i,25] <- 'sussex hate crime'
}
}
x_1 <- count(x, "force")
colnames(x_1)[2] <- "Number Reported"
y <- merge(police_access, police_report, by = 'report_id', all.x = TRUE)
y$force <- gsub(" \\[UK\\]", "", y$force)
y$where_you_live[is.na(y$where_you_live)] <- 0
for (i in 1:nrow(y)) {
if (y[i,16] == 1) {
y[i,25] <- 'metropolitan hate crime'
} else if (y[i,16] == 2) {
y[i,25] <- 'sussex hate crime'
}
}
y_1 <- count(y, "force")
colnames(y_1)[2] <- "Number Accessed"
access_data <- merge(x_1, y_1, by = "force", all.x = TRUE)
access_data[is.na(access_data)] <- 0
percent1 <- round((access_data$`Number Accessed`/access_data$`Number Reported`) * 100)
access_data$`Percent Accessed`  <- paste(percent1, '%', sep = "")
access_data <- access_data[order(-access_data[,3]),]
access_data <- access_data[!grepl('^$', access_data$force),]
x$created.y <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", x$created.y)
for (i in 1:nrow(x)) {
if (x[i, 10] >= new_quarter) {
x[i,10] <- "Quarter to Date"
} else if (old_quarter <= x[i, 10] & x[i,10] < new_quarter) {
x[i,10] <- "Past Quarter"
} else {
x[i,10] <- "All-Time"
}
}
present_quarter <- x[grep("Quarter to Date", x$created.y),]
report1 <- count(present_quarter, "force")
colnames(report1)[2] <- "Reports Quarter to Date"
present_access <- present_quarter[!is.na(present_quarter$accessed),]
access1 <- count(present_access, "force")
colnames(access1)[2] <- "Accessed Quarter to Date"
to.date.data <- merge(report1, access1, by = "force", all.x = TRUE)
to.date.data[is.na(to.date.data)] <- 0
percent2 <- round((to.date.data$`Accessed Quarter to Date`/to.date.data$`Reports Quarter to Date`) * 100)
to.date.data$`Percent Accessed`  <- paste(percent2, '%', sep = "")
to.date.data <- to.date.data[order(-to.date.data[,3]),]
to.date.data <- to.date.data[!grepl('^$', to.date.data$force),]
new.data <- merge(access_data, to.date.data, by = 'force', all = TRUE)
past_quarter <- x[grep("Past Quarter", x$created.y),]
report2 <- count(past_quarter, "force")
colnames(report2)[2] <- "Reports Past Quarter"
past_access <- past_quarter[!is.na(past_quarter$accessed),]
access2 <- count(past_access, "force")
colnames(access2)[2] <- "Accessed Past Quarter"
past.data <- merge(report2, access2, by = "force", all.x = TRUE)
past.data[is.na(past.data)] <- 0
percent3 <- round((past.data$`Accessed Past Quarter`/past.data$`Reports Past Quarter`) * 100)
past.data$`Percent Accessed`  <- paste(percent3, '%', sep = "")
past.data <- past.data[order(-past.data[,3]),]
past.data <- past.data[!grepl('^$', past.data$force),]
full_accessed_data <- merge(new.data, past.data, by = 'force', all = TRUE)
full_accessed_data$`Number Not Accessed` <- full_accessed_data$`Number Reported` - full_accessed_data$`Number Accessed`
percent4 <- round((full_accessed_data$`Number Not Accessed` / full_accessed_data$`Number Reported`) * 100)
full_accessed_data$`Percent Not Accessed`  <- paste(percent4, '%', sep = "")
new_merge <- merge(police_access, police_report, by = "report_id", all.y = TRUE)
#new_merge$created.y <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$created.y)
new_merge$created.y <- as.POSIXct(new_merge$created.y)
#new_merge$accessed <- gsub(" ([0123456789]{2}:[0123456789]{2}:[0123456789]{2})$", "", new_merge$accessed)
new_merge$accessed <- as.POSIXct(new_merge$accessed)
new_merge$force <- gsub(" \\[UK\\]", "", new_merge$force)
for (i in 1:nrow(new_merge)) {
if (new_merge[i,16] == 1) {
new_merge[i,25] <- 'metropolitan hate crime'
} else if (new_merge[i,16] == 2) {
new_merge[i,25] <- 'sussex hate crime'
}
}
new_merge$`access time` <- new_merge$accessed-new_merge$created.y
time.avg <- aggregate(`access time`~ force, data = new_merge, FUN = function(x) median = median(x))
time.avg$`access time` <- round(time.avg$`access time`)
colnames(time.avg)[2] <- "Time til Accessed (Minutes)"
full_accessed_data <- merge(full_accessed_data, time.avg, by = "force", all = TRUE)
full_accessed_data <- full_accessed_data[order(-full_accessed_data[,2]),]
colnames(full_accessed_data)[c(4,7,10)] <- c("Percent Accessed", "Percent Accessed Quarter to Date", "Percent Accessed Past Quarter")
full_accessed_data[is.na(full_accessed_data[,c(2,3,5,6,8,9,11)])] <- 0
full_accessed_data[is.na(full_accessed_data[,c(4,7,10)])] <- "0%"
full_accessed_data[,c(2,3,5,6,8,9,11)][is.na(full_accessed_data[,c(2,3,5,6,8,9,11)])] <- 0
full_accessed_data[,c(4,7,10)][is.na(full_accessed_data[,c(4,7,10)])] <- "0%"
grep("", full_accessed_data$force)
grep("^$", full_accessed_data$force)
full_accessed_data <- full_accessed_data[-grep("^$", full_accessed_data$force),]
setwd("/Users/toddvogel/Documents/Senior Year/Stat 159/stat159-fall2016-project3/code/scripts")
data_2015 <- read.csv("../../data/MERGED2014_15_PP.csv")
for (i in 1:ncol(data_2015)) {
if (is.factor(data_2015[,i]) == TRUE) {
data_2015[,i] <- as.numeric(levels(data_2015[,i]))[data_2015[,i]]
}
}
##### remove columns with greater than 50% of data missing
data_2015 <- data_2015[, colSums(is.na(data_2015)) <= .5 * nrow(data_2015)]
##### scaling and mean centering data
scaled_data_2015 <- scale(data_2015, center = TRUE, scale = TRUE)
scaled_data_2015 <- cbind( data_2015[, c(1,2)], scaled_data_2015[,c(-1,-2)])
for(i in 1:ncol(scaled_data_2015)){
scaled_data_2015[is.na(scaled_data_2015[,i]), i] <- median(scaled_data_2015[,i], na.rm = TRUE)
}
scaled_data_2015 <- scaled_data_2015[, colSums(is.na(scaled_data_2015)) <= .5 * nrow(scaled_data_2015)]
write.csv(scaled_data_2015, file = "../../data/scaled_data_2015.csv")
scaled_data_2015 <- read.csv("../../data/scaled_data_2015.csv")
set.seed(1)
training_data <- scaled_data_2015[sample(nrow(scaled_data_2015), round(.75 * nrow(scaled_data_2015))),]
sort(training_data$X)
training_data <- training_data[,-1]
write.csv(training_data, file = "../../data/training_data.csv")
set.seed(1)
test_data <- scaled_data_2015[-sample(nrow(scaled_data_2015), round(.75 * nrow(scaled_data_2015))),]
test_data <- test_data[,-1]
write.csv(test_data, file = "../../data/test_data.csv")
data_2015 <- read.csv("../../data/MERGED2014_15_PP.csv")
ncol(data_2015)
ncol(scaled_data_2015)
for (i in 1:ncol(data_2015)) {
if (is.factor(data_2015[,i]) == TRUE) {
data_2015[,i] <- as.numeric(levels(data_2015[,i]))[data_2015[,i]]
}
}
ncol(data_2015)
View(data_2015)
data_2015$LN_MEDIAN_HH_INC
data_2015 <- read.csv("../../data/MERGED2014_15_PP.csv")
data_2015$LN_MEDIAN_HH_INC
data_2015$MN_EARN_WNE_P10
data_2015$MN_EARN_WNE_INC3_P10
data_2015$MGT_25K_P6
data_2015$GT_25K_P7
setwd("/Users/toddvogel/Documents/Senior Year/Stat 159/stat159-fall2016-project3/code/scripts")
data_2006 <- read.csv("../../data/MERGED2005_06_PP.csv")
View(data_2006)
data_2006$LN_MEDIAN_HH_INC
for (i in 1:ncol(data_2006)) {
if (is.factor(data_2006[,i]) == TRUE) {
data_2006[,i] <- as.numeric(levels(data_2006[,i]))[data_2006[,i]]
}
}
data_2006 <- data_2006[, colSums(is.na(data_2006)) <= .5 * nrow(data_2006)]
ncol(data_2006)
scaled_data_2006 <- scale(data_2006, center = TRUE, scale = TRUE)
scaled_data_2006 <- cbind( data_2006[, c(1,2)], scaled_data_2006[,c(-1,-2)])
for(i in 1:ncol(scaled_data_2006)){
scaled_data_2006[is.na(scaled_data_2006[,i]), i] <- median(scaled_data_2006[,i], na.rm = TRUE)
}
scaled_data_2006 <- scaled_data_2006[, colSums(is.na(scaled_data_2006)) <= .5 * nrow(scaled_data_2006)]
write.csv(scaled_data_2006, file = "../../data/scaled_data_2006.csv")
scaled_data_2006 <- read.csv("../../data/scaled_data_2006.csv")
set.seed(1)
training_data <- scaled_data_2006[sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
sort(training_data$X)
training_data <- training_data[,-1]
write.csv(training_data, file = "../../data/training_data.csv")
set.seed(1)
test_data <- scaled_data_2006[-sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
test_data <- test_data[,-1]
write.csv(test_data, file = "../../data/test_data.csv")
setwd("/Users/toddvogel/Documents/Senior Year/Stat 159/stat159-fall2016-project3/code/scripts")
data_2006 <- read.csv("../../data/MERGED2005_06_PP.csv")
for (i in 1:ncol(data_2006)) {
if (is.factor(data_2006[,i]) == TRUE) {
data_2006[,i] <- as.numeric(levels(data_2006[,i]))[data_2006[,i]]
}
}
##### remove columns with greater than 50% of data missing
data_2006 <- data_2006[, colSums(is.na(data_2006)) <= .5 * nrow(data_2006)]
##### scaling and mean centering data
scaled_data_2006 <- scale(data_2006, center = TRUE, scale = TRUE)
scaled_data_2006 <- cbind( data_2006[, c(1,2)], scaled_data_2006[,c(-1,-2)])
for(i in 1:ncol(scaled_data_2006)){
scaled_data_2006[is.na(scaled_data_2006[,i]), i] <- median(scaled_data_2006[,i], na.rm = TRUE)
}
scaled_data_2006 <- scaled_data_2006[, colSums(is.na(scaled_data_2006)) <= .5 * nrow(scaled_data_2006)]
write.csv(scaled_data_2006, file = "../../data/scaled_data_2006.csv")
scaled_data_2006 <- read.csv("../../data/scaled_data_2006.csv")
set.seed(1)
training_data <- scaled_data_2006[sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
sort(training_data$X)
training_data <- training_data[,-1]
write.csv(training_data, file = "../../data/training_data.csv")
set.seed(1)
test_data <- scaled_data_2006[-sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
test_data <- test_data[,-1]
write.csv(test_data, file = "../../data/test_data.csv")
setwd("/Users/toddvogel/Documents/Senior Year/Stat 159/stat159-fall2016-project3/code/scripts")
data_2015 <- read.csv("../../data/MERGED2014_15_PP.csv")
for (i in 1:ncol(data_2015)) {
if (is.factor(data_2015[,i]) == TRUE) {
data_2015[,i] <- as.numeric(levels(data_2015[,i]))[data_2015[,i]]
}
}
##### remove columns with greater than 50% of data missing
data_2015 <- data_2015[, colSums(is.na(data_2015)) <= .5 * nrow(data_2015)]
##### scaling and mean centering data
scaled_data_2015 <- scale(data_2015, center = TRUE, scale = TRUE)
scaled_data_2015 <- cbind( data_2015[, c(1,2)], scaled_data_2015[,c(-1,-2)])
for(i in 1:ncol(scaled_data_2015)){
scaled_data_2015[is.na(scaled_data_2015[,i]), i] <- median(scaled_data_2015[,i], na.rm = TRUE)
}
scaled_data_2015 <- scaled_data_2015[, colSums(is.na(scaled_data_2015)) <= .5 * nrow(scaled_data_2015)]
write.csv(scaled_data_2015, file = "../../data/scaled_data_2015.csv")
setwd("/Users/toddvogel/Documents/Senior Year/Stat 159/stat159-fall2016-project3/code/scripts")
data_2006 <- read.csv("../../data/MERGED2005_06_PP.csv")
for (i in 1:ncol(data_2006)) {
if (is.factor(data_2006[,i]) == TRUE) {
data_2006[,i] <- as.numeric(levels(data_2006[,i]))[data_2006[,i]]
}
}
##### remove columns with greater than 50% of data missing
data_2006 <- data_2006[, colSums(is.na(data_2006)) <= .5 * nrow(data_2006)]
##### scaling and mean centering data
scaled_data_2006 <- scale(data_2006, center = TRUE, scale = TRUE)
scaled_data_2006 <- cbind( data_2006[, c(1,2)], scaled_data_2006[,c(-1,-2)])
for(i in 1:ncol(scaled_data_2006)){
scaled_data_2006[is.na(scaled_data_2006[,i]), i] <- median(scaled_data_2006[,i], na.rm = TRUE)
}
scaled_data_2006 <- scaled_data_2006[, colSums(is.na(scaled_data_2006)) <= .5 * nrow(scaled_data_2006)]
write.csv(scaled_data_2006, file = "../../data/scaled_data_2006.csv")
scaled_data_2006 <- read.csv("../../data/scaled_data_2006.csv")
set.seed(1)
training_data <- scaled_data_2006[sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
sort(training_data$X)
training_data <- training_data[,-1]
write.csv(training_data, file = "../../data/training_data.csv")
set.seed(1)
test_data <- scaled_data_2006[-sample(nrow(scaled_data_2006), round(.75 * nrow(scaled_data_2006))),]
test_data <- test_data[,-1]
write.csv(test_data, file = "../../data/test_data.csv")
training_data <- read.csv(file = "../../data/training_data.csv")
View(training_data)
colnames(training_data)
colnames(training_data[,-training_data$LN_MEDIAN_HH_INC])
colnames(training_data[,training_data$LN_MEDIAN_HH_INC])
colnames(training_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC"])
colnames(training_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")])
library("glmnet")
library("lars")
library("MASS")
training_data <- read.csv(file = "../../data/training_data.csv")
training_data <- training_data[,-1]
#formatting response and predictors
response <- training_data$LN_MEDIAN_HH_INC #Log of median income
response <- as.matrix(response)
predictors <- training_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]  #everythning but median income
predictors <- as.matrix(predictors)
grid <- 10^seq(10, -2, length = 100)
set.seed(100)
cross_v <- cv.glmnet(x = predictors, y = response, intercept = FALSE, standardize = FALSE, lambda = grid, alpha = 0)
best_model_ridge <- coef(cross_v, cross_v$lambda.min)
#saving coefficients of the model
save(best_model_ridge, file = "../../data/ridge_model.RData")
#Adding Histograms to Images
png('../../images/CV_Errors_Ridge.png')
plot(cross_v, main = "CV Errors Ridge")
dev.off()
test_set <- read.csv(file = "../../data/test_data.csv")
response = test_set$LN_MEDIAN_HH_INC
test_set <- test_set[,-1]
test_set <- test_set[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]
test_predictors = as.matrix(test_set)
test_ridge <- predict(cross_v, newx = test_predictors, s = "lambda.min", type="response")
save(test_ridge,file =  "../../data/testing_ridge.RData")
source("../functions/mse_function.R")
MSE_ridge = MSE(test_ridge, response)
save(MSE_ridge, file = "../../data/MSE_ridge.RData" )
full_data <- read.csv(file = "../../data/scaled_credit.csv")
full_data <- full_data[,-1]
response <- full_data$LN_MEDIAN_HH_INC #median income
response <- as.matrix(response)
predictors <- full_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]  #everythning but median income
predictors <- as.matrix(predictors)
#rerunning model on the full data set
full_ridge = glmnet(x = predictors, y = response, intercept = FALSE, standardize = FALSE, lambda = cross_v$lambda.min, alpha = 0)
#getting coefficients and saving
ridge_coef <- coef(full_ridge)
save(ridge_coef, file = "../../data/full_coeffecients_ridge.RData")
#saving data from this model to a txt file
sink(file = "../../data/ridge_model.txt")
best_model_ridge
print("Coefficients for the Ridge Model")
best_model_ridge
print("MSE for the Ridge Model")
MSE_ridge
print("Coefficients for the model run on the full data set")
ridge_coef
sink()
full_data <- read.csv(file = "../../data/scaled_data_2006.csv")
full_data <- full_data[,-1]
response <- full_data$LN_MEDIAN_HH_INC #median income
response <- as.matrix(response)
predictors <- full_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]  #everythning but median income
predictors <- as.matrix(predictors)
#rerunning model on the full data set
full_ridge = glmnet(x = predictors, y = response, intercept = FALSE, standardize = FALSE, lambda = cross_v$lambda.min, alpha = 0)
#getting coefficients and saving
ridge_coef <- coef(full_ridge)
save(ridge_coef, file = "../../data/full_coeffecients_ridge.RData")
#saving data from this model to a txt file
sink(file = "../../data/ridge_model.txt")
best_model_ridge
print("Coefficients for the Ridge Model")
best_model_ridge
print("MSE for the Ridge Model")
MSE_ridge
print("Coefficients for the model run on the full data set")
ridge_coef
sink()
training_data <- read.csv("../../data/training_data.csv")
training_data <- training_data[,-1]
library("glmnet")
library("lars")
response <- training_data$LN_MEDIAN_HH_INC
response <- as.matrix(response)
predictors <- training_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]
predictors <- as.matrix(predictors)
grid <- 10^seq(10, -2, length = 100)
set.seed(1)
lm.lasso <- cv.glmnet(predictors, response, lambda = grid, alpha = 1, intercept = FALSE, standardize = FALSE)
bestmodel_lasso <- coef(lm.lasso, lm.lasso$lambda.min)
save(bestmodel_lasso, file = "../../data/lasso_model.RData")
png("../../images/CV_errors_lasso.png")
plot(lm.lasso, main = "CV Errors Lasso")
dev.off()
test_data <- read.csv("../../data/test_data.csv")
test_data <- test_data[,-1]
test_predictors <- test_data[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]
test_predictors <- as.matrix(test_predictors)
test_response <- test_data$LN_MEDIAN_HH_INC
predicted_response <- predict(lm.lasso, newx=test_predictors, s = "lambda.min", type = "response")
predicted_response <- as.vector(predicted_response)
source("../functions/mse_function.R")
lasso_mse <- MSE(predicted_response, test_response)
save(lasso_mse, file = "../../data/MSE_lasso.RData")
scaled_data_2006 <- read.csv("../../data/scaled_data_2006.csv")
scaled_data_2006 <- scaled_data_2006$LN_MEDIAN_HH_INC
total_response <- scaled_data_2006$Balance
total_response <- as.matrix(total_response)
total_predictors <- scaled_data_2006[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]
total_predictors <- as.matrix(total_predictors)
full.lasso <- glmnet(total_predictors, total_response, lambda = lm.lasso$lambda.min, alpha = 1, intercept = FALSE, standardize = FALSE)
full_coefficients <- coef(full.lasso)
save(full_coefficients, file = "../../data/full_coefficients_lasso.RData")
sink("../../data/lasso_model.txt")
print("model coefficients")
bestmodel_lasso
print("prediction mse")
lasso_mse
print("full model coefficients")
full_coefficients
sink()
scaled_data_2006 <- read.csv("../../data/scaled_data_2006.csv")
scaled_data_2006 <- scaled_data_2006[,-1]
total_response <- scaled_data_2006$LN_MEDIAN_HH_INC
total_response <- as.matrix(total_response)
total_predictors <- scaled_data_2006[,-which(names(training_data) == "LN_MEDIAN_HH_INC")]
total_predictors <- as.matrix(total_predictors)
full.lasso <- glmnet(total_predictors, total_response, lambda = lm.lasso$lambda.min, alpha = 1, intercept = FALSE, standardize = FALSE)
full_coefficients <- coef(full.lasso)
save(full_coefficients, file = "../../data/full_coefficients_lasso.RData")
sink("../../data/lasso_model.txt")
print("model coefficients")
bestmodel_lasso
print("prediction mse")
lasso_mse
print("full model coefficients")
full_coefficients
sink()
